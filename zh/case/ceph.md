## ceph 介绍

Proxmox已经对proxmox ve添加了对ceph的支持。

在pxvirt中，除了ceph的源不同，其他基本无差异。

## ceph的硬件要求

本文仅针对ceph rbd 和 cephfs场景

### 网络
    
    - 交换机： 优先使用10G的数据中心交换机，支持rdma优先。双交换机满足冗余。
    - 网卡：优先使用10G的网卡，支持rdma优先。 2张双口或者单张双口，使用lacp满足冗余要求
    - 线缆：优先使用aoc有源光缆，10G以上。

### 硬盘
    - 数量：ceph的osd磁盘越多，性能越好，因为如果是单块4t，不如4块1t。每个节点应该最少4块磁盘做osd。
    - 缓存盘： 
        - 类型：对于机械盘来说，建议配备一张或者两张 nvme固态做缓存，可以得到更好的性能。
        - 容量：建议为10%磁盘的总容量。考虑到如果读写不频繁，有个1-5%也可以。但是如果包含了ceph的db和wal，那么容量需要稍微大点如20%。
    - 类型: nvme>scsi>sata。 对于机械来讲，转速越快性能越好，缓存越大，性能越好。

### 阵列卡
    ceph 需要直接管理硬盘。因此一般不需要阵列卡
    - jbod： 阵列卡配置成jbod或者使用直通卡
    - 单盘raid0：如果阵列卡带有缓存，可将每块硬盘做成单盘raid0的vd。这样虽然增加了写入延迟，但因为缓存的介入，性能大幅度会提升。（不推荐）

### CPU
    
    应该为每个osd 准备1个2G+主频以上的cpu核心。

### 内存

    - OSD: 每个OSD启动之后就会使用约3-6G的内存。随着读写的增加，内存仍然会上涨。建议最低满足1T数据->1G内存的配置。例如4块16T的osd，那么 4 * （6+16）=88G的内存。osd内存也可以做限制，请参考
    - MDS/MGR： 针对监视器节点，每个进程大概会使用4G-10G的内存。